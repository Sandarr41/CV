# HW4 — Дистилляция на CIFAR-10

Скрипт `distillation.py` реализует три варианта дистилляции между **Учителем** и **Студентом**:

1. **Эксперимент 1 (дистилляция логитов)**  
   Студент обучается по сумме:
   - `CrossEntropy(student_logits, y)`
   - `KLDiv(student_logits/T, teacher_logits/T)`

2. **Эксперимент 2 (совпадение скрытого состояния без обучения новых блоков)**  
   Дополнительно к логит-дистилляции:
   - извлекаются `forward_features` у учителя и студента;
   - фичи приводятся к одной размерности через `adaptive_avg_pool2d(..., 1)` + обрезка до `min(C_student, C_teacher)`;
   - добавляется `cosine loss = 1 - cos(student_feat, teacher_feat)`.

3. **Эксперимент 3 (обучаемый регрессор)**  
   Добавляется обучаемый блок:
   - `Conv2d(1x1)` регрессор, проецирующий фичи студента в размерность каналов учителя;
   - после проекции размер карты приводится к размеру учителя через `adaptive_avg_pool2d`;
   - в функцию потерь добавляется `MSE(student_proj_feat, teacher_feat)`.

Во всех экспериментах Учитель работает в режиме `eval()` и без обновления градиентов (`torch.no_grad()`).


## Структура кода

- `hw4/distillation.py` — точка входа (оркестрация запусков).
- `hw4/cli.py` — CLI-аргументы.
- `hw4/trainer.py` — `DistillationTrainer`, лоссы и `train_epoch`.
- `hw4/experiments.py` — запуск одного эксперимента и summary-таблица.
- `hw4/results.py` — сохранение результатов в `.json`/`.csv`.

## Как получить метрики по каждому эксперименту

### Вариант 1: запустить один конкретный эксперимент

```bash
python -m hw4.distillation --experiment 1 --epochs 10
python -m hw4.distillation --experiment 2 --epochs 10 --feature-weight 1.0
python -m hw4.distillation --experiment 3 --epochs 10 --feature-weight 1.0
```

Для каждого запуска скрипт печатает:
- baseline метрики Teacher/Student до дистилляции;
- метрики по эпохам;
- финальные метрики Student после дистилляции;
- краткую итоговую строку summary.

### Вариант 2: запустить все эксперименты сразу и получить сводку

```bash
python -m hw4.distillation --experiment all --epochs 10 --results-path hw4/results.json
```

Или в CSV:

```bash
python -m hw4.distillation --experiment all --epochs 10 --results-path hw4/results.csv
```

В режиме `--experiment all` скрипт:
- последовательно запускает эксперименты 1, 2 и 3;
- печатает общую таблицу сравнения;
- сохраняет метрики в `.json` или `.csv` (по расширению `--results-path`).

## Полезные аргументы

- `--teacher` / `--student` — архитектуры timm (по умолчанию: `efficientnet_b0` и `mobilenetv3_small_100`);
- `--teacher-checkpoint` / `--student-checkpoint` — инициализация из чекпоинтов;
- `--alpha` — вес логит-дистилляции;
- `--temperature` — температура для KL-дистилляции;
- `--feature-weight` — вес feature-loss (cosine/MSE);
- `--labeled-fraction` — доля размеченных данных CIFAR-10;
- `--results-path` — путь для сохранения метрик (`.json` или `.csv`).


## Результаты экспериментов

Ниже приведены результаты запуска трёх режимов дистилляции (10 эпох) и итоговая сводка.

### Эксперимент 1 — дистилляция логитов

- **Baseline до обучения**:
  - Teacher: `loss=4.8654`, `acc=9.39`, `f1=0.0796`
  - Student: `loss=6.8968`, `acc=11.52`, `f1=0.0843`
- **Финал после дистилляции**:
  - Student*: `loss=1.5843`, `acc=34.24`, `f1=0.3307`
- **Лучшие значения по ходу обучения**:
  - `best_acc=37.98` (5 эпоха), `best_f1=0.3678` (5 эпоха)

### Эксперимент 2 — логиты + cosine loss по признакам

- **Baseline до обучения**:
  - Teacher: `loss=5.5496`, `acc=11.15`, `f1=0.0666`
  - Student: `loss=7.3126`, `acc=9.85`, `f1=0.0785`
- **Финал после дистилляции**:
  - Student*: `loss=1.9681`, `acc=24.29`, `f1=0.2263`
- **Лучшие значения по ходу обучения**:
  - `best_acc=30.69` (8 эпоха), `best_f1=0.2892` (8 эпоха)

### Эксперимент 3 — логиты + обучаемый регрессор (Conv1x1) + MSE

- **Baseline до обучения**:
  - Teacher: `loss=5.7781`, `acc=11.80`, `f1=0.0797`
  - Student: `loss=6.4900`, `acc=9.59`, `f1=0.0773`
- **Финал после дистилляции**:
  - Student*: `loss=2.2061`, `acc=23.08`, `f1=0.1827`
- **Лучшие значения по ходу обучения**:
  - `best_acc=29.94` (3 эпоха), `best_f1=0.2549` (3 эпоха)

### Итоговая сводная таблица

| exp | before_acc | after_acc | gain_acc | before_f1 | after_f1 | gain_f1 | best_acc | best_f1 |
|---:|---:|---:|---:|---:|---:|---:|---:|---:|
| 1 | 11.52 | **34.24** | **22.72** | 0.0843 | **0.3307** | **0.2464** | **37.98** | **0.3678** |
| 2 | 9.85 | 24.29 | 14.44 | 0.0785 | 0.2263 | 0.1479 | 30.69 | 0.2892 |
| 3 | 9.59 | 23.08 | 13.49 | 0.0773 | 0.1827 | 0.1054 | 29.94 | 0.2549 |

### Краткий разбор результатов

1. **Лучший результат показал Эксперимент 1 (только дистилляция логитов)** — максимальный прирост и по accuracy, и по macro-F1.
2. Добавление feature loss в текущей конфигурации (**Эксперименты 2 и 3**) улучшает студента относительно baseline, но уступает логит-дистилляции без feature matching.
3. Для **Эксперимента 3** видно раннее достижение пика (около 3 эпохи) и последующее ухудшение, что может указывать на необходимость более аккуратной настройки `feature_weight`, `alpha`, `temperature`, либо применения early stopping.
4. Во всех трёх случаях дистилляция существенно поднимает качество студента относительно старта.
