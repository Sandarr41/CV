# ДЗ 2: Детекция аномалий автоэнкодером

## Описание задачи

Задача: **детекция аномалий автоэнкодером**.

Имеются изображения технологического процесса разлива металлических цилиндров. Есть риск нарушения технологии: когда стенки цилиндра не успевают застывать и трескаются, незастывший металл выливается, не образуя требуемую заготовку. Необходимо оперативно определить лунку, где произошел пролив.

### Классы
- **Пролив (аномалия)** — `1`
- **Не пролив (норма)** — `0`

---

## Датасет

### Характеристики датасета
- **Train:** `10,000` изображений без проливов (нормальные состояния)
- **Proliv:** `154` изображения с проливами (аномалии)
- **Test:** `3,794` изображения (перемешанные проливы и не-проливы)
  - Нормальные (`0`): `3,665`
  - Аномальные (`1`): `129`

### Размер изображений
- Исходный размер: примерно `48×30` пикселей (очень маленькие изображения)
- Обработка: `Resize` до `64×64` (bilinear interpolation)

---

## Архитектура: сверточный автоэнкодер

## Общая концепция
Автоэнкодер обучается **только на нормальных изображениях** (`train`). Идея:

1. Модель учится хорошо реконструировать нормальные изображения (низкая ошибка реконструкции).
2. На аномальных изображениях (проливах) ошибка реконструкции становится выше.
3. Задаётся порог: если `error > threshold`, то это аномалия (пролив).

### ConvAutoencoder

#### Encoder (сжатие)
- `Conv2d: 1 → 32`, `kernel=3`, `stride=2`
- `BatchNorm2d + ReLU`
- `Conv2d: 32 → 64`, `kernel=3`, `stride=2`
- `BatchNorm2d + ReLU`
- `Conv2d: 64 → 128`, `kernel=3`, `stride=2`
- `BatchNorm2d + ReLU`
- `Flatten → Linear(128×8×8 → 512)`

#### Decoder (восстановление)
- `Linear(512 → 128×8×8)`
- `ConvTranspose2d: 128 → 64`, `kernel=4`, `stride=2`
- `BatchNorm2d + ReLU`
- `ConvTranspose2d: 64 → 32`, `kernel=4`, `stride=2`
- `BatchNorm2d + ReLU`
- `ConvTranspose2d: 32 → 1`, `kernel=4`, `stride=2`
- `Tanh()` → выход в диапазоне `[-1, 1]`

### Параметры модели
- **Latent dimension:** `512`
- **Вход:** `64×64×1` (grayscale)
- **Выход:** `64×64×1` (grayscale)

---

## Метрики оценки

Для детекции аномалий ключевые метрики:

- **TPR (True Positive Rate / Recall):** доля правильно обнаруженных проливов среди всех проливов
  - Цель: `> 95%`
- **TNR (True Negative Rate / Specificity):** доля правильно классифицированных нормальных состояний
  - Цель: `> 95%`

### Дополнительные метрики
- **TP (True Positive):** проливы, правильно обнаруженные
- **TN (True Negative):** нормальные, правильно классифицированные
- **FP (False Positive):** нормальные, ошибочно классифицированные как пролив
- **FN (False Negative):** проливы, пропущенные

---

## Подход к оценке аномалий

Базовый скор по умолчанию — `MSE` между входом и реконструкцией.
Дополнительно оставлены альтернативы:

```python
score = MSE(x, x_hat)         # SCORE_TYPE='mse'
score = L1(x, x_hat)          # SCORE_TYPE='l1'
score = α·L1 + β·(1 - SSIM)   # SCORE_TYPE='hybrid'
```

### Подбор порога
Используется метод `tnr_target`:

1. Считаются ошибки реконструкции для `train` (норма) и `proliv` (аномалии).
2. Перебираются пороги, обеспечивающие `TNR >= TARGET_TNR` (по умолчанию `0.95`) на `train`.
3. Среди таких порогов выбирается порог с максимальным `TPR` на `proliv`.

---

## Запуск обучения

### 1. Установка зависимостей
```bash
cd hw2
pip install -r requirements.txt
```

### 2. Запуск
```bash
python main.py
```

Скрипт автоматически:
- загрузит данные из `dataset/`;
- обучит автоэнкодер на `train` (только нормальные изображения);
- подберёт порог на основе `train` и `proliv`;
- оценит модель на `test`.

---

## Параметры обучения

Основные параметры эксперимента:

- **Batch size:** `128`
- **Epochs:** `80` (с early stopping после `12` эпох без улучшений)
- **Learning rate:** `3e-4`
- **Optimizer:** `Adam` (`weight_decay=1e-5`)
- **Loss:** `MSE`
- **Score:** `MSE`
- **Device:** `MPS`

---

## Структура проекта

```text
hw2/
├── datasets.py            # Dataset и DataLoader
├── evaluate.py            # Оценка реконструкции, порог, метрики
├── main.py                # Скрипт обучения и оценки
├── model.py               # Архитектура автоэнкодера
├── train.py               # Цикл обучения
├── README.md              # Документация
└── dataset/               # Датасет
    ├── train/
    ├── proliv/
    └── test/
```

---

## Результаты экспериментов

### Итоговые значения
- **Optimized threshold:** `0.0002403111`
- **Mean normal error:** `6.3250845e-05`
- **Mean proliv error:** `0.0008732571`
- **TPR (пролив):** `0.8682`
- **TNR (норма):** `0.8816`

### Сводная таблица

| Показатель | Значение |
|---|---:|
| Optimized threshold | 0.0002403111 |
| Mean normal error | 6.3250845e-05 |
| Mean proliv error | 0.0008732571 |
| TPR (пролив) | 0.8682 |
| TNR (норма) | 0.8816 |

### Краткий вывод
- Порог успешно разделяет нормальные и аномальные случаи по ошибке реконструкции.
- Ошибка на аномалиях значительно выше, чем на норме.
- Текущие значения `TPR` и `TNR` высокие, но пока не достигают целевого уровня `95%`.
